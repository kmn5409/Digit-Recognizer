{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot ad hoc mnist instances\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from numpy import genfromtxt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load (downloaded if needed) the MNIST dataset\n",
    "train = pd.read_csv('./datasets/train.csv')\n",
    "test = pd.read_csv('./datasets/test.csv')\n",
    "#y_train = np.array(df['label'].values)\n",
    "#print(y_train)\n",
    "k = 0\n",
    "X_train = (train.iloc[:,1:].values.astype('float32'))\n",
    "y_train = (train.iloc[:,0].values.astype('int32'))\n",
    "y_train = np.tile(y_train, (1, 1))\n",
    "X_test = test.values.astype('float32')\n",
    "# X_train = X_train.reshape(X_train.shape[0], 28, 28)\n",
    "#net = NeuralNet([784, 30, 10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network:\n",
    "    def __init__(self,X,y):\n",
    "        self.m = X_train.shape[0]\n",
    "        self.input_layer_size = X_train.shape[1]\n",
    "        self.hidden_layer_size = 25\n",
    "        self.output_layer_size = 10\n",
    "        self.epsilon = []\n",
    "        self.J = 0\n",
    "        self.grad = 0\n",
    "        self.num_labels = 10\n",
    "        self.lambda1 = 0\n",
    "        self.iter = 0\n",
    "        \n",
    "        \n",
    "    def randInit(self):\n",
    "        theta1 = []\n",
    "        theta2 = []\n",
    "        for x in range(2):\n",
    "            if(x == 0):\n",
    "                L_in = self.input_layer_size\n",
    "                L_out = self.hidden_layer_size\n",
    "            if(x == 1):\n",
    "                L_in = self.hidden_layer_size\n",
    "                L_out = self.output_layer_size\n",
    "            self.epsilon.append(math.sqrt(6) / math.sqrt(L_in + L_out))\n",
    "            \n",
    "        X_bias = self.input_layer_size + 1\n",
    "        \n",
    "        theta1 = (np.random.rand(self.hidden_layer_size,\n",
    "                 self.input_layer_size + 1) * (2 * self.epsilon[0])\n",
    "                 - self.epsilon[0])\n",
    "        theta2 = (np.random.rand(self.output_layer_size,\n",
    "                  self.hidden_layer_size + 1) * (2 * self.epsilon[1])\n",
    "                  - self.epsilon[1]) \n",
    "        #self.theta1 = np.random.randn(self.hidden_layer_size, self.input_layer_size+1)\n",
    "        #self.theta2 = np.random.randn(self.output_layer_size, self.hidden_layer_size+1)\n",
    "        print(theta2[0][:2])\n",
    "        return theta1,theta2\n",
    "    \n",
    "    \n",
    "    def sigmoid(self,z):        \n",
    "        g = 1.0 / (1.0 + np.exp(-z))\n",
    "        return g\n",
    "    \n",
    "    def sigmoidGradient(self,z):\n",
    "        g = 1.0 / (1.0 + np.exp(-z))\n",
    "        g = g*(1-g)\n",
    "        return g\n",
    "    \n",
    "    def nnCostFunction(self,nn_params, input_layer_size, hidden_layer_size, \\\n",
    "                       num_labels, X, y, lambda_reg):\n",
    "        \n",
    "        self.J = 0;\n",
    "        self.iter+=1;\n",
    "\n",
    "        \n",
    "        theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)], \\\n",
    "                     (hidden_layer_size, input_layer_size + 1), order='F')\n",
    "\n",
    "        theta2 = np.reshape(nn_params[hidden_layer_size * (input_layer_size + 1):], \\\n",
    "                     (num_labels, hidden_layer_size + 1), order='F')\n",
    "        \n",
    "        Theta1_grad = np.zeros( theta1.shape )\n",
    "        Theta2_grad = np.zeros( theta2.shape )\n",
    "        \n",
    "        a1 = np.column_stack((np.ones((self.m,1)), X))\n",
    "        \n",
    "        z2 = np.dot(a1, theta1.T)\n",
    "        \n",
    "        a2 = self.sigmoid(z2)\n",
    "        \n",
    "        a2 = np.column_stack((np.ones((a2.shape[0],1)), a2))\n",
    "        \n",
    "        z3 = np.dot(a2, theta2.T)\n",
    "        \n",
    "        a3 = self.sigmoid(z3)\n",
    "        #print(a3.shape,\"Here\")\n",
    "        #print(a3[0][:])\n",
    "        \n",
    "#         yMatrix = (np.tile([y for y in range(10)], (self.m, 1) ) = \n",
    "#         np.tile(y, (1,self.num_labels) ))\n",
    "\n",
    "        numLabels_temp = y\n",
    "        #numLabels_temp = np.tile([y for y in range(10)], (self.m, 1) )\n",
    "        \n",
    "        #yMatrix_temp = np.tile(y, (1,self.num_labels) )\n",
    "        yMatrix = y\n",
    "        yMatrix = np.zeros((self.m,self.num_labels))\n",
    "        \n",
    "        for i in range(self.m):\n",
    "            yMatrix[i, numLabels_temp[i]-1] = 1\n",
    "             \n",
    "        #yMatrix = np.equal(numLabels_temp, yMatrix_temp)\n",
    "        #print( (yMatrix * ((np.log(a3)))).shape, \"First\" )\n",
    "        #print( (((1 - yMatrix.T).T * np.log(1 - a3))).shape )\n",
    "        cost = 0\n",
    "        \n",
    "        for i in range(self.m):\n",
    "            cost +=  np.sum( yMatrix[i] * np.log(a3[i]) + (1 - yMatrix[i]) * np.log(1 - a3[i]) )\n",
    "        #cost =  ( (yMatrix * (np.log(a3)) ) + ((1 - yMatrix.T).T * np.log(1 - a3)) )\n",
    "        #print(yMatrix.shape, (np.log(a3)).shape, \"Here\" )\n",
    "        #print(cost.shape,\"Cost\")\n",
    "        #print(cost[0])\n",
    "        #print(self.theta1.shape)\n",
    "        #print( (self.theta1[:,1:]).shape)\n",
    "        #print(self.theta1[0][1])\n",
    "        #sqTheta1 = np.square(theta1[:,1:]);\n",
    "        sqTheta1 = np.sum(np.sum(theta1[:,1:]**2))\n",
    "        sqTheta2 = np.sum(np.sum(theta2[:,1:]**2))\n",
    "        #print(sqTheta2[0][0])\n",
    "        #print(self.theta2.shape)\n",
    "        #sqTheta2 = np.square(theta2[:,1:]);\n",
    "        #print(sqTheta2.shape);\n",
    "        \n",
    "        self.J = -(1.0/self.m) * cost\n",
    "        \n",
    "        self.J = self.J + ( (lambda_reg/(2.0*self.m)) *(sqTheta1 + sqTheta2) )\n",
    "            \n",
    "        print( str(self.iter) + \") \",self.J,\" cost\")\n",
    "        \n",
    "        #Back Prop\n",
    "        delta1 = 0;\n",
    "        delta2 = 0; \n",
    "        #print(a1.shape,\"a1.shape\")\n",
    "        for t in range(0,self.m):\n",
    "            #print(a1.shape)\n",
    "            a1_t = a1[t,]\n",
    "            #print(a1_t.shape)\n",
    "            #print(a1_t)\n",
    "            \n",
    "            a2_t = a2[t,].T\n",
    "            \n",
    "            a3_t = a3[t,].T\n",
    "            \n",
    "            y_output_t = yMatrix[t,].T\n",
    "            \n",
    "            delta3_t = (a3_t - y_output_t);\n",
    "            \n",
    "            z2_t = np.dot(a1_t, theta1.T);\n",
    "            #print(self.theta1.shape, \"self.theta1\")\n",
    "            z2_t= np.insert(z2_t,0 ,1, axis=0)\n",
    "            #print(z2_t.shape)\n",
    "            \n",
    "            delta2_t = np.dot(theta2.T, delta3_t) * self.sigmoidGradient(z2_t)\n",
    "            \n",
    "            delta2_t = delta2_t[1:]\n",
    "            \n",
    "            delta2 = delta2 + np.outer(delta3_t, a2_t)\n",
    "            delta1 = delta1 + np.outer(delta2_t, a1_t.T)\n",
    "        \n",
    "        #theta1_no_bias = (self.lambda1/self.m) * self.theta1[:][2:]\n",
    "        #theta2_no_bias = (self.lambda1/self.m) * self.theta2[:][2:]\n",
    "        \n",
    "        Theta1_grad = delta1 / self.m\n",
    "        Theta2_grad = delta2 / self.m\n",
    "        \n",
    "        Theta1_grad_unregularized = np.copy(Theta1_grad)\n",
    "        Theta2_grad_unregularized = np.copy(Theta2_grad)\n",
    "        Theta1_grad += (float(self.lambda1)/self.m)*theta1\n",
    "        Theta2_grad += (float(self.lambda1)/self.m)*theta2\n",
    "        Theta1_grad[:,0] = Theta1_grad_unregularized[:,0]\n",
    "        Theta2_grad[:,0] = Theta2_grad_unregularized[:,0]\n",
    "        self.grad = np.concatenate((Theta1_grad.reshape(Theta1_grad.size, order='F'), Theta2_grad.reshape(Theta2_grad.size, order='F')))\n",
    "        return self.J,self.grad\n",
    "        \n",
    "    def train(self,theta1,theta2,nn_params, X, y, lambda_reg):\n",
    "        print(self.J,)\n",
    "        print('Training Neural Network...')\n",
    "        #maxiter = 20\n",
    "        #maxiter = 30\n",
    "        maxiter = 100\n",
    "        #maxiter = 5\n",
    "        lambda_reg = 0.1\n",
    "        nn_params = np.concatenate((theta1.reshape(theta1.size, order='F'), theta2.reshape(theta2.size, order='F')))\n",
    "        myargs = (self.input_layer_size, self.hidden_layer_size, self.num_labels, X, y, lambda_reg)\n",
    "        results = minimize(self.nnCostFunction, x0=nn_params, args=myargs, options={'disp': True, 'maxiter':maxiter}, method=\"L-BFGS-B\", jac=True)\n",
    "\n",
    "        nn_params = results[\"x\"]\n",
    "\n",
    "        # Obtain Theta1 and Theta2 back from nn_params\n",
    "        Theta1 = np.reshape(nn_params[:self.hidden_layer_size * (self.input_layer_size + 1)], \\\n",
    "                         (self.hidden_layer_size, self.input_layer_size + 1), order='F')\n",
    "\n",
    "        Theta2 = np.reshape(nn_params[self.hidden_layer_size * (self.input_layer_size + 1):], \\\n",
    "                         (self.num_labels, self.hidden_layer_size + 1), order='F')\n",
    "\n",
    "        print('Program paused. Press enter to continue.\\n')\n",
    "        return Theta1,Theta2\n",
    "        \n",
    "    def predict(self,Theta1, Theta2, X):\n",
    "    #PREDICT Predict the label of an input given a trained neural network\n",
    "    #   p = PREDICT(Theta1, Theta2, X) outputs the predicted label of X given the\n",
    "    #   trained weights of a neural network (Theta1, Theta2)\n",
    "\n",
    "        # turns 1D X array into 2D\n",
    "        if X.ndim == 1:\n",
    "            X = np.reshape(X, (-1,X.shape[0]))\n",
    "\n",
    "        # Useful values\n",
    "        m = X.shape[0]\n",
    "        num_labels = Theta2.shape[0]\n",
    "\n",
    "        # You need to return the following variables correctly \n",
    "        p = np.zeros((m,1))\n",
    "\n",
    "        h1 = self.sigmoid( np.dot( np.column_stack( ( np.ones((m,1)), X ) ) , Theta1.T ) )\n",
    "        h2 = self.sigmoid( np.dot( np.column_stack( ( np.ones((m,1)), h1) ) , Theta2.T ) )\n",
    "\n",
    "        p = np.argmax(h2, axis=1)\n",
    "\n",
    "        # =========================================================================\n",
    "\n",
    "        return p + 1# offsets python's zero notation\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.35533692  0.11811196]\n",
      "(25, 785)\n",
      "(10, 26)\n",
      "784\n"
     ]
    }
   ],
   "source": [
    "nn = Neural_Network(X_train,np.transpose(y_train))\n",
    "theta1 = theta2 = []\n",
    "theta1,theta2 = nn.randInit()\n",
    "print(theta1.shape)\n",
    "print(theta2.shape)\n",
    "print(X_train.shape[1])\n",
    "nn_params = np.concatenate((theta1.reshape(theta1.size, order='F'), theta2.reshape(theta2.size, order='F')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#J, cost = nn.nnCostFunction(nn_params,784,25,10,X_train,np.transpose(y_train),0)\n",
    "#print(J.shape)\n",
    "#print('Training Set Accuracy: {:f}\\n(this value should be about 0.287629)'.format(J))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 42000)\n",
      "0\n",
      "Training Neural Network...\n",
      "1)  7.547805963175655  cost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keanu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2)  4.336238842719597  cost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keanu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:46: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3)  3.7422240093387784  cost\n",
      "4)  3.27965179737502  cost\n",
      "5)  3.085600115286401  cost\n",
      "6)  3.028745855476156  cost\n",
      "7)  2.9752147427986566  cost\n",
      "8)  2.9168386495977514  cost\n",
      "9)  2.8191922208949496  cost\n",
      "10)  2.7458207706066817  cost\n",
      "11)  2.6715331343303728  cost\n",
      "12)  2.544431442082077  cost\n",
      "13)  2.282745721381331  cost\n",
      "14)  1.9397086249902347  cost\n",
      "15)  1.84045861722335  cost\n",
      "16)  1.7699099859764813  cost\n",
      "17)  1.695495856608408  cost\n",
      "18)  1.6392858500039171  cost\n",
      "19)  1.5980085505501025  cost\n",
      "20)  1.5430261124412576  cost\n",
      "21)  1.4911097441381587  cost\n",
      "22)  1.5531600118143312  cost\n",
      "23)  1.4634456266067217  cost\n",
      "24)  1.4216447109713284  cost\n",
      "25)  1.3838714791242888  cost\n",
      "26)  1.335902154760237  cost\n",
      "27)  1.3117339328843003  cost\n",
      "28)  1.2638311480270346  cost\n",
      "29)  1.2244258440086357  cost\n",
      "30)  1.2357880837942914  cost\n",
      "31)  1.2183425055937283  cost\n",
      "32)  1.2060208241459596  cost\n",
      "33)  1.1977218839972164  cost\n",
      "34)  1.1922268263890932  cost\n",
      "35)  1.1757114128818904  cost\n",
      "36)  1.1620518263827664  cost\n",
      "37)  1.1594085341183393  cost\n",
      "38)  1.1504595807182707  cost\n",
      "39)  1.1352208601402032  cost\n",
      "40)  1.1301490117886541  cost\n",
      "41)  1.1157894766887442  cost\n",
      "42)  1.1059408457057551  cost\n",
      "43)  1.0986794728316482  cost\n",
      "44)  1.0986325849306415  cost\n",
      "45)  1.086979073795096  cost\n",
      "46)  1.0794191875350516  cost\n",
      "47)  1.0717033562335403  cost\n",
      "48)  1.0743581526375792  cost\n",
      "49)  1.0719311413435364  cost\n",
      "50)  1.0711746367841009  cost\n",
      "51)  1.068187863677926  cost\n",
      "52)  1.074933267630151  cost\n",
      "53)  1.0646964567512986  cost\n",
      "54)  1.0708888441453237  cost\n",
      "55)  1.0623982910615435  cost\n",
      "56)  1.060337666685212  cost\n",
      "57)  1.048946643922423  cost\n",
      "58)  1.0421502801442828  cost\n",
      "59)  1.0339957401972943  cost\n",
      "60)  1.0287362999096796  cost\n",
      "61)  1.0314979098125432  cost\n",
      "62)  1.0283598538493712  cost\n",
      "63)  1.0183023975525634  cost\n",
      "64)  1.0757313453057653  cost\n",
      "65)  1.0006196157081253  cost\n",
      "66)  1.0047091513561461  cost\n",
      "67)  0.9997963546333251  cost\n",
      "68)  0.9950582136214341  cost\n",
      "69)  0.9949678420029313  cost\n",
      "70)  0.9989624679759647  cost\n",
      "71)  0.9959953173862292  cost\n",
      "72)  0.9947551958096419  cost\n",
      "73)  0.9913604704691601  cost\n",
      "74)  0.9850242348791178  cost\n",
      "75)  0.97965790943703  cost\n",
      "76)  0.974777383585922  cost\n",
      "77)  0.9737663064207195  cost\n",
      "78)  0.972739034819852  cost\n",
      "79)  0.9682426582290303  cost\n",
      "80)  0.9462140751496871  cost\n",
      "81)  0.929644997882511  cost\n",
      "82)  0.9211251649384745  cost\n",
      "83)  0.9200687841242209  cost\n",
      "84)  0.9176505069855386  cost\n",
      "85)  0.9062137898749185  cost\n",
      "86)  0.9160410115108084  cost\n",
      "87)  0.9034501672762173  cost\n",
      "88)  0.8997682707100332  cost\n",
      "89)  0.8949837488555419  cost\n",
      "90)  0.8940234204210132  cost\n",
      "91)  0.8949159859285164  cost\n",
      "92)  0.8930798164315025  cost\n",
      "93)  0.891118730354112  cost\n",
      "94)  0.8880806879708466  cost\n",
      "95)  0.8897797667004264  cost\n",
      "96)  0.8889128464504342  cost\n",
      "97)  0.8877497067845916  cost\n",
      "98)  0.8863337888115254  cost\n",
      "99)  0.8850156235290236  cost\n",
      "100)  0.8825824287388673  cost\n",
      "101)  0.8768779794527694  cost\n",
      "102)  0.8758669551932893  cost\n",
      "103)  0.8762607188055204  cost\n",
      "104)  0.875081531832323  cost\n",
      "105)  0.8739942205116012  cost\n",
      "106)  0.8690228375733909  cost\n",
      "107)  0.8664686095406148  cost\n",
      "108)  0.8638221945370448  cost\n",
      "109)  0.8645761480881956  cost\n",
      "110)  0.863217943996673  cost\n",
      "111)  0.8611542008009463  cost\n",
      "112)  0.8591238000131947  cost\n",
      "113)  0.8577347601615891  cost\n",
      "114)  0.8551532892070264  cost\n",
      "115)  0.8531338950592287  cost\n",
      "116)  0.850455447447705  cost\n",
      "117)  0.8534640630837879  cost\n",
      "118)  0.8494403139570823  cost\n",
      "119)  0.8477410990902782  cost\n",
      "120)  0.8523172508006475  cost\n",
      "121)  0.8465474135649474  cost\n",
      "122)  0.8478744840490356  cost\n",
      "123)  0.8465151919002183  cost\n",
      "124)  0.8462317422738058  cost\n",
      "125)  0.8445371768249478  cost\n",
      "Program paused. Press enter to continue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_params = np.concatenate((theta1.reshape(theta1.size, order='F'), theta2.reshape(theta2.size, order='F')))\n",
    "print(y_train.shape)\n",
    "#nn.nnCostFunction(0,0,0,0,X_train,np.transpose(y_train),0)\n",
    "theta1,theta2 = nn.train(theta1,theta2,nn_params,X_train,np.transpose(y_train),0)\n",
    "pred = nn.predict(theta1, theta2, X_train)\n",
    "\n",
    "# uncomment code below to see the predictions that don't match\n",
    "# fmt = '{}   {}'\n",
    "# print(fmt.format('y', 'pred'))\n",
    "# for y_elem, pred_elem in zip(y, pred):\n",
    "#     if y_elem != pred_elem:\n",
    "#         print(fmt.format(y_elem%10, pred_elem%10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Training Set Accuracy: 86.642857\n"
     ]
    }
   ],
   "source": [
    "k = 0\n",
    "print(pred[k])\n",
    "for i in pred:\n",
    "    if(i == 10):\n",
    "        pred[k] = 0\n",
    "    k+=1\n",
    "print('Training Set Accuracy: {:f}'.format( ( np.mean(pred == y_train)*100 ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28000,)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keanu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: RuntimeWarning: overflow encountered in exp\n"
     ]
    }
   ],
   "source": [
    "ans = nn.predict(theta1,theta2, X_test)\n",
    "print(ans.shape)\n",
    "print(type(ans))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "k = 0\n",
    "print(ans[1])\n",
    "for i in ans:\n",
    "    if(i == 10):\n",
    "        ans[k] = 0\n",
    "    k+=1\n",
    "print(ans[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ImageId  Label\n",
      "0        1      2\n",
      "1        2      0\n",
      "2        3      9\n",
      "3        4      7\n",
      "4        5      2\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(data=ans,columns=[\"Label\"])\n",
    "file = \"submission.csv\"\n",
    "df.reset_index(level=0, inplace=True)\n",
    "#df['ImageId'] = df.index\n",
    "df.columns = ['ImageId', 'Label']\n",
    "for i in df['ImageId']:\n",
    "    df['ImageId'][i]+=1\n",
    "#for\n",
    "print(df[:5])\n",
    "df.to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc9da693048>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIgAAACFCAYAAACAJLCMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAACCdJREFUeJztnV1oVVcWx/+rcSKKD07UijaxqShi\nH8QBrUp9GBjFWNAOgpAgIlgoiIUZnYcaq+A3QXBArCCR0VTUFD8GLPpQS2gtIzLYgrTaaI1CNX40\nRJFRxI/onoecbvc+zVk53ntyzj3m/4OLa5+1z90L/LPX2fvusyLGGBASxWtZB0BKGwqEqFAgRIUC\nISoUCFGhQIgKBUJUihKIiNSIyCURaRORVUkFRUoHKXSjTETKAPwMYDaAdgBnAdQZY35KLjySNQOK\nuPcdAG3GmKsAICKfA3gfQKRARITbtqVDpzFmRG+dikkxbwC47rTbg2skH/wSp1MxM4j0cO13M4SI\nfAjgwyLGIRlSjEDaAVQ57UoAN8OdjDGNABoBppg8UkyKOQtgvIi8JSLlAGoBfJFMWKRUKHgGMcZ0\nichHAL4EUAZgjzHmQmKRkZKg4GVuQYMxxZQS3xtjpvTWiTupRIUCISoUCFGhQIgKBUJUKBCiQoEQ\nFQqEqFAgRIUCISoUCFEp5ud+AqCmpsZr379/39qnT59OO5zE4QxCVCgQosIU0wNlZWVee+7cudZe\ns2aN55s6darXfvLkibVv374de8z9+/dbe8OGDZ7v6dOnsb8naTiDEBUKhKhQIESFRw4D5syZY+2N\nGzd6vilTej2ZlygnT5702gsXLrS2u4wuEh45JMVDgRCVfptihg4d6rXPnDlj7QkTJsT+ntbWVq99\n6NAha0+aNCnyvhEj/NdiZ86cGdl3xYoV1t6+fXvs2HqBKYYUDwVCVCgQotJvt9oHDx7stQcOHBjZ\n9+rVq9Y+fPiw59u2bZvX7uzsLGj8c+fOWXvcuHGez93qT/AZJBacQYhKrwIRkT0i0iEi551rFSLy\nlYhcDv79Y9+GSbIiToppAvApgH3OtVUAWowxDUHxulUAPk4+vL7j5k2/lMmsWbOsPWPGDM934MCB\nxMevr6/32uG0Uir0OoMYY74FcDd0+X0AnwX2ZwD+mnBcpEQo9CF1pDHmFgAYY26JyOtRHVmCKt/0\n+SqGJajyTaEC+VVERgWzxygAHUkGlQXuUta1k8TdXl+2bFns+44ePdoX4cSi0GXuFwCWBPYSAMeS\nCYeUGnGWuc0AzgCYICLtIvIBgAYAs0XkMrorLTf0bZgkK3pNMcaYugjXXxKO5ZUjvDvrLqUrKioi\n7wunlH379kX07Hu4k0pUKBCiQoEQlX71a+7o0aOtXVVVpfT0uXLlirXj/loLACtXrvTamzdvjnXf\n+vXrvfbjx49jj5k0nEGICgVCVF65FDN9+nRrh0szLF261NqVlZWxv9PdWb1z547n2717t9ceNmyY\ntVevXh17jLq6F7sJFy6UTsl7ziBEhQIhKhQIUcnli1PuFvbevXs934IFC6xdXl6exHCJ8fz5c2tv\n2bLF87lL22fPnqURDl+cIsVDgRAVCoSo5GIfZNCgQV67sbHR2rW1tZH3hfcsLl68aO2WlhbPN3Hi\nRGtPmzbN840ZMyZ+sArulrm7fQ/4z1UPHz5MZLwk4AxCVCgQopKLFBPeMl+0aFFk33v37ll73bp1\nnm/nzp2xxmtubvbaSaUYN1WGl+duWgm//5slnEGICgVCVCgQopKLZ5B58+bF7uue4jpx4oTnq66u\ntvbatWs9n7tcDi+rNcKnvx49emRtd+kMAIsXL7a2iHi+gwcPWrujw38P7dSpU9YeMmSI53vw4EHs\nWAuBMwhRoUCISi5SzMiRIyN94V+j58+fb+2tW7d6vuHDh8car6ury2sfOXLE2uEl6PHjx7229pcZ\n7t59UUXDLW0J+H9hYseOHZ6vra3N2mPHjvV8kydPjhwvCTiDEJU47+ZWicjXItIqIhdE5G/BdZah\n6gfEmUG6APzDGDMRwHQAy0XkbbwoQzUeQEvQJq8YL32iTESOobtm2acA/uzUCPnGGKPWsC70RFk4\nJy9fvryQr1Fxf10Nv+DU1NSUyBjuSfpr164V9B3hk/INDQUXVoh1ouylHlJFpBrAnwD8FzHLULEE\nVb6JLRARGQLgKIC/G2P+F97oiYIlqPJNLIGIyB/QLY4Dxph/B5dTK0MV/lXWPfz7Mrus7hJ0165d\nns9dyl6/fv0lI4zHjRs3rB1errrjDxjg/7ds2rTJ2mmXo4qzihEA/wLQaoz5p+NiGap+QJwZ5F0A\niwH8KCK/FRRfje6yU4eCklTXACyMuJ/kmDglqP4DIOqBg2WoXnFy+eIUSQS+OEWKhwIhKhQIUaFA\niAoFQlQoEKJCgRAVCoSoUCBEhQIhKhQIUaFAiAoFQlQoEKJCgRAVCoSoUCBEhQIhKhQIUaFAiEra\n9UE6AfwCYHhglwL9NZY343RK9VS7HVTkuzgnqtOAsegwxRAVCoSoZCWQxt67pAZjUcjkGYTkB6YY\nokKBEJVUBSIiNSJySUTaRCT1onciskdEOkTkvHMtk2qNeakemZpARKQMwE4AcwG8DaAuqJaYJk0A\nakLXsqrWmI/qkcaYVD4AZgD40mnXA6hPa3xn3GoA5532JQCjAnsUgEtpxxSMfQzA7FKJ57dPminm\nDQBu8a/24FrWeNUaAfRYrbEv0apHZhGPS5oC6alKUb9fY4erR2YdT5g0BdIOoMppVwK4meL4Ufwa\nVGlEX1drDKNVj8winp5IUyBnAYwXkbdEpBxALborJWZNJtUac1M9MuUHsfcA/AzgCoBPMngQbAZw\nC8BTdM9oHwAYhu7VwuXg34qUYpmJ7hT7A4Bzwee9rOKJ+nCrnahwJ5WoUCBEhQIhKhQIUaFAiAoF\nQlQoEKLyf1feCz025z8jAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc9da277978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "digit = X_test.reshape(X_test.shape[0],28,28)\n",
    "print(ans[0])\n",
    "plt.subplot(221)\n",
    "plt.imshow(digit[i], cmap=plt.get_cmap('gray'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
