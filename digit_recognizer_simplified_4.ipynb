{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot ad hoc mnist instances\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import genfromtxt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load (downloaded if needed) the MNIST dataset\n",
    "train = pd.read_csv('./datasets/train.csv')\n",
    "test = pd.read_csv('./datasets/test.csv')\n",
    "#y_train = np.array(df['label'].values)\n",
    "#print(y_train)\n",
    "k = 0\n",
    "X_train = (train.iloc[:,1:].values.astype('float32'))\n",
    "y_train = (train.iloc[:,0].values.astype('int32'))\n",
    "y_train = np.tile(y_train, (1, 1))\n",
    "X_test = test.values.astype('float32')\n",
    "# X_train = X_train.reshape(X_train.shape[0], 28, 28)\n",
    "#net = NeuralNet([784, 30, 10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network:\n",
    "    def __init__(self,X,y):\n",
    "        self.m = X_train.shape[0]\n",
    "        self.input_layer_size = X_train.shape[1]\n",
    "        self.hidden_layer_size = 25\n",
    "        self.output_layer_size = 10\n",
    "        self.epsilon = []\n",
    "        self.J = 0\n",
    "        self.grad = 0\n",
    "        self.num_labels = 10\n",
    "        self.lambda1 = 0\n",
    "        \n",
    "        \n",
    "    def randInit(self):\n",
    "        theta1 = []\n",
    "        theta2 = []\n",
    "        for x in range(2):\n",
    "            if(x == 0):\n",
    "                L_in = self.input_layer_size\n",
    "                L_out = self.hidden_layer_size\n",
    "            if(x == 1):\n",
    "                L_in = self.hidden_layer_size\n",
    "                L_out = self.output_layer_size\n",
    "            self.epsilon.append(math.sqrt(6) / math.sqrt(L_in + L_out))\n",
    "            \n",
    "        X_bias = self.input_layer_size + 1\n",
    "        \n",
    "        theta1 = (np.random.rand(self.hidden_layer_size,\n",
    "                 self.input_layer_size + 1) * (2 * self.epsilon[0])\n",
    "                 - self.epsilon[0])\n",
    "        theta2 = (np.random.rand(self.output_layer_size,\n",
    "                  self.hidden_layer_size + 1) * (2 * self.epsilon[1])\n",
    "                  - self.epsilon[1]) \n",
    "        #self.theta1 = np.random.randn(self.hidden_layer_size, self.input_layer_size+1)\n",
    "        #self.theta2 = np.random.randn(self.output_layer_size, self.hidden_layer_size+1)\n",
    "        print(theta2[0][:2])\n",
    "        return theta1,theta2\n",
    "    \n",
    "    \n",
    "    def sigmoid(self,z):        \n",
    "        g = 1.0 / (1.0 + np.exp(-z))\n",
    "        return g\n",
    "    \n",
    "    def sigmoidGradient(self,z):\n",
    "        g = 1.0 / (1.0 + np.exp(-z))\n",
    "        g = g*(1-g)\n",
    "        return g\n",
    "    \n",
    "    def nnCostFunction(self,nn_params, input_layer_size, hidden_layer_size, \\\n",
    "                       num_labels, X, y, lambda_reg):\n",
    "        \n",
    "        self.J = 0;\n",
    "\n",
    "        \n",
    "        theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)], \\\n",
    "                     (hidden_layer_size, input_layer_size + 1), order='F')\n",
    "\n",
    "        theta2 = np.reshape(nn_params[hidden_layer_size * (input_layer_size + 1):], \\\n",
    "                     (num_labels, hidden_layer_size + 1), order='F')\n",
    "        \n",
    "        Theta1_grad = np.zeros( theta1.shape )\n",
    "        Theta2_grad = np.zeros( theta2.shape )\n",
    "        \n",
    "        a1 = np.column_stack((np.ones((self.m,1)), X))\n",
    "        \n",
    "        z2 = np.dot(a1, theta1.T)\n",
    "        \n",
    "        a2 = self.sigmoid(z2)\n",
    "        \n",
    "        a2 = np.column_stack((np.ones((a2.shape[0],1)), a2))\n",
    "        \n",
    "        z3 = np.dot(a2, theta2.T)\n",
    "        \n",
    "        a3 = self.sigmoid(z3)\n",
    "        #print(a3.shape,\"Here\")\n",
    "        #print(a3[0][:])\n",
    "        \n",
    "#         yMatrix = (np.tile([y for y in range(10)], (self.m, 1) ) = \n",
    "#         np.tile(y, (1,self.num_labels) ))\n",
    "\n",
    "        numLabels_temp = y\n",
    "        #numLabels_temp = np.tile([y for y in range(10)], (self.m, 1) )\n",
    "        \n",
    "        #yMatrix_temp = np.tile(y, (1,self.num_labels) )\n",
    "        yMatrix = y\n",
    "        yMatrix = np.zeros((self.m,self.num_labels))\n",
    "        \n",
    "        for i in range(self.m):\n",
    "            yMatrix[i, numLabels_temp[i]-1] = 1\n",
    "             \n",
    "        #yMatrix = np.equal(numLabels_temp, yMatrix_temp)\n",
    "        #print( (yMatrix * ((np.log(a3)))).shape, \"First\" )\n",
    "        #print( (((1 - yMatrix.T).T * np.log(1 - a3))).shape )\n",
    "        cost = 0\n",
    "        \n",
    "        for i in range(self.m):\n",
    "            cost +=  np.sum( yMatrix[i] * np.log(a3[i]) + (1 - yMatrix[i]) * np.log(1 - a3[i]) )\n",
    "        #cost =  ( (yMatrix * (np.log(a3)) ) + ((1 - yMatrix.T).T * np.log(1 - a3)) )\n",
    "        #print(yMatrix.shape, (np.log(a3)).shape, \"Here\" )\n",
    "        #print(cost.shape,\"Cost\")\n",
    "        #print(cost[0])\n",
    "        #print(self.theta1.shape)\n",
    "        #print( (self.theta1[:,1:]).shape)\n",
    "        #print(self.theta1[0][1])\n",
    "        #sqTheta1 = np.square(theta1[:,1:]);\n",
    "        sqTheta1 = np.sum(np.sum(theta1[:,1:]**2))\n",
    "        sqTheta2 = np.sum(np.sum(theta2[:,1:]**2))\n",
    "        #print(sqTheta2[0][0])\n",
    "        #print(self.theta2.shape)\n",
    "        #sqTheta2 = np.square(theta2[:,1:]);\n",
    "        #print(sqTheta2.shape);\n",
    "        \n",
    "        self.J = -(1.0/self.m) * cost\n",
    "        \n",
    "        self.J = self.J + ( (lambda_reg/(2.0*self.m)) *(sqTheta1 + sqTheta2) )\n",
    "            \n",
    "        print(self.J,\" cost\")\n",
    "        \n",
    "        #Back Prop\n",
    "        delta1 = 0;\n",
    "        delta2 = 0; \n",
    "        #print(a1.shape,\"a1.shape\")\n",
    "        for t in range(0,self.m):\n",
    "            #print(a1.shape)\n",
    "            a1_t = a1[t,]\n",
    "            #print(a1_t.shape)\n",
    "            #print(a1_t)\n",
    "            \n",
    "            a2_t = a2[t,].T\n",
    "            \n",
    "            a3_t = a3[t,].T\n",
    "            \n",
    "            y_output_t = yMatrix[t,].T\n",
    "            \n",
    "            delta3_t = (a3_t - y_output_t);\n",
    "            \n",
    "            z2_t = np.dot(a1_t, theta1.T);\n",
    "            #print(self.theta1.shape, \"self.theta1\")\n",
    "            z2_t= np.insert(z2_t,0 ,1, axis=0)\n",
    "            #print(z2_t.shape)\n",
    "            \n",
    "            delta2_t = np.dot(theta2.T, delta3_t) * self.sigmoidGradient(z2_t)\n",
    "            \n",
    "            delta2_t = delta2_t[1:]\n",
    "            \n",
    "            delta2 = delta2 + np.outer(delta3_t, a2_t)\n",
    "            delta1 = delta1 + np.outer(delta2_t, a1_t.T)\n",
    "        \n",
    "        #theta1_no_bias = (self.lambda1/self.m) * self.theta1[:][2:]\n",
    "        #theta2_no_bias = (self.lambda1/self.m) * self.theta2[:][2:]\n",
    "        \n",
    "        Theta1_grad = delta1 / self.m\n",
    "        Theta2_grad = delta2 / self.m\n",
    "        \n",
    "        Theta1_grad_unregularized = np.copy(Theta1_grad)\n",
    "        Theta2_grad_unregularized = np.copy(Theta2_grad)\n",
    "        Theta1_grad += (float(self.lambda1)/self.m)*theta1\n",
    "        Theta2_grad += (float(self.lambda1)/self.m)*theta2\n",
    "        Theta1_grad[:,0] = Theta1_grad_unregularized[:,0]\n",
    "        Theta2_grad[:,0] = Theta2_grad_unregularized[:,0]\n",
    "        self.grad = np.concatenate((Theta1_grad.reshape(Theta1_grad.size, order='F'), Theta2_grad.reshape(Theta2_grad.size, order='F')))\n",
    "        print(\"hey\")\n",
    "        return self.J,self.grad\n",
    "        \n",
    "    def train(self,theta1,theta2,nn_params, X, y, lambda_reg):\n",
    "        print(self.J,)\n",
    "        print('Training Neural Network...')\n",
    "        #maxiter = 20\n",
    "        maxiter = 30\n",
    "        #maxiter = 5\n",
    "        lambda_reg = 0.1\n",
    "        nn_params = np.concatenate((theta1.reshape(theta1.size, order='F'), theta2.reshape(theta2.size, order='F')))\n",
    "        myargs = (self.input_layer_size, self.hidden_layer_size, self.num_labels, X, y, lambda_reg)\n",
    "        results = minimize(self.nnCostFunction, x0=nn_params, args=myargs, options={'disp': True, 'maxiter':maxiter}, method=\"L-BFGS-B\", jac=True)\n",
    "\n",
    "        nn_params = results[\"x\"]\n",
    "\n",
    "        # Obtain Theta1 and Theta2 back from nn_params\n",
    "        Theta1 = np.reshape(nn_params[:self.hidden_layer_size * (self.input_layer_size + 1)], \\\n",
    "                         (self.hidden_layer_size, self.input_layer_size + 1), order='F')\n",
    "\n",
    "        Theta2 = np.reshape(nn_params[self.hidden_layer_size * (self.input_layer_size + 1):], \\\n",
    "                         (self.num_labels, self.hidden_layer_size + 1), order='F')\n",
    "\n",
    "        print('Program paused. Press enter to continue.\\n')\n",
    "        return Theta1,Theta2\n",
    "        \n",
    "    def predict(self,Theta1, Theta2, X):\n",
    "    #PREDICT Predict the label of an input given a trained neural network\n",
    "    #   p = PREDICT(Theta1, Theta2, X) outputs the predicted label of X given the\n",
    "    #   trained weights of a neural network (Theta1, Theta2)\n",
    "\n",
    "        # turns 1D X array into 2D\n",
    "        if X.ndim == 1:\n",
    "            X = np.reshape(X, (-1,X.shape[0]))\n",
    "\n",
    "        # Useful values\n",
    "        m = X.shape[0]\n",
    "        num_labels = Theta2.shape[0]\n",
    "\n",
    "        # You need to return the following variables correctly \n",
    "        p = np.zeros((m,1))\n",
    "\n",
    "        h1 = self.sigmoid( np.dot( np.column_stack( ( np.ones((m,1)), X ) ) , Theta1.T ) )\n",
    "        h2 = self.sigmoid( np.dot( np.column_stack( ( np.ones((m,1)), h1) ) , Theta2.T ) )\n",
    "\n",
    "        p = np.argmax(h2, axis=1)\n",
    "\n",
    "        # =========================================================================\n",
    "\n",
    "        return p + 1 # offsets python's zero notation\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.14648157 -0.14246265]\n",
      "(25, 785)\n",
      "(10, 26)\n",
      "784\n"
     ]
    }
   ],
   "source": [
    "nn = Neural_Network(X_train,np.transpose(y_train))\n",
    "theta1 = theta2 = []\n",
    "theta1,theta2 = nn.randInit()\n",
    "print(theta1.shape)\n",
    "print(theta2.shape)\n",
    "print(X_train.shape[1])\n",
    "nn_params = np.concatenate((theta1.reshape(theta1.size, order='F'), theta2.reshape(theta2.size, order='F')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keanu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:41: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.856421222775882  cost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keanu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:45: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey\n"
     ]
    }
   ],
   "source": [
    "J, cost = nn.nnCostFunction(nn_params,784,25,10,X_train,np.transpose(y_train),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "Training Set Accuracy: 8.856421\n",
      "(this value should be about 0.287629)\n"
     ]
    }
   ],
   "source": [
    "print(J.shape)\n",
    "print('Training Set Accuracy: {:f}\\n(this value should be about 0.287629)'.format(J))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 42000)\n",
      "8.856421222775882\n",
      "Training Neural Network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keanu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:41: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.856494354166106  cost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keanu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:45: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey\n",
      "5.387091410623823  cost\n",
      "hey\n",
      "4.532914848369086  cost\n",
      "hey\n",
      "3.192250369044284  cost\n",
      "hey\n",
      "3.12094336583484  cost\n",
      "hey\n",
      "3.0504120579473906  cost\n",
      "hey\n",
      "2.9727079143048143  cost\n",
      "hey\n",
      "2.887843239824388  cost\n",
      "hey\n",
      "2.793329578308898  cost\n",
      "hey\n",
      "2.694821180038742  cost\n",
      "hey\n",
      "2.590649257342193  cost\n",
      "hey\n",
      "2.4537121800337713  cost\n",
      "hey\n",
      "2.143763480009159  cost\n",
      "hey\n",
      "2.023858325107074  cost\n",
      "hey\n",
      "1.9188534855315194  cost\n",
      "hey\n",
      "1.885825414233746  cost\n",
      "hey\n",
      "1.8482691297693397  cost\n",
      "hey\n",
      "1.7703553979115902  cost\n",
      "hey\n",
      "1.7001767169488147  cost\n",
      "hey\n",
      "1.644828987090602  cost\n",
      "hey\n",
      "1.5984894933340374  cost\n",
      "hey\n",
      "1.5481516534058197  cost\n",
      "hey\n",
      "1.5051706104246794  cost\n",
      "hey\n",
      "1.5165218520285686  cost\n",
      "hey\n",
      "1.4809362166362736  cost\n",
      "hey\n",
      "1.4683009552483315  cost\n",
      "hey\n",
      "1.4409368570207375  cost\n",
      "hey\n",
      "1.374494961817111  cost\n",
      "hey\n",
      "1.323768086496216  cost\n",
      "hey\n",
      "1.2828143472790037  cost\n",
      "hey\n",
      "1.234348309393479  cost\n",
      "hey\n",
      "1.2069227328422072  cost\n",
      "hey\n",
      "1.1920246439680415  cost\n",
      "hey\n",
      "1.1926873225172807  cost\n",
      "hey\n",
      "1.1893017121803853  cost\n",
      "hey\n",
      "Program paused. Press enter to continue.\n",
      "\n",
      "Training Set Accuracy: 70.897619\n"
     ]
    }
   ],
   "source": [
    "nn_params = np.concatenate((theta1.reshape(theta1.size, order='F'), theta2.reshape(theta2.size, order='F')))\n",
    "print(y_train.shape)\n",
    "#nn.nnCostFunction(0,0,0,0,X_train,np.transpose(y_train),0)\n",
    "theta1,theta2 = nn.train(theta1,theta2,nn_params,X_train,np.transpose(y_train),0)\n",
    "pred = nn.predict(theta1, theta2, X_train)\n",
    "\n",
    "# uncomment code below to see the predictions that don't match\n",
    "# fmt = '{}   {}'\n",
    "# print(fmt.format('y', 'pred'))\n",
    "# for y_elem, pred_elem in zip(y, pred):\n",
    "#     if y_elem != pred_elem:\n",
    "#         print(fmt.format(y_elem%10, pred_elem%10))\n",
    "\n",
    "print('Training Set Accuracy: {:f}'.format( ( np.mean(pred == y_train)*100 ) ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
