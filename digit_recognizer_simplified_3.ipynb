{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot ad hoc mnist instances\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import genfromtxt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load (downloaded if needed) the MNIST dataset\n",
    "train = pd.read_csv('./datasets/train.csv')\n",
    "test = pd.read_csv('./datasets/test.csv')\n",
    "#y_train = np.array(df['label'].values)\n",
    "#print(y_train)\n",
    "k = 0\n",
    "X_train = (train.iloc[:,1:].values.astype('float32'))\n",
    "y_train = (train.iloc[:,0].values.astype('int32'))\n",
    "y_train = np.tile(y_train, (1, 1))\n",
    "X_test = test.values.astype('float32')\n",
    "# X_train = X_train.reshape(X_train.shape[0], 28, 28)\n",
    "#net = NeuralNet([784, 30, 10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network:\n",
    "    def __init__(self,X,y):\n",
    "        self.m = X_train.shape[0]\n",
    "        self.input_layer_size = X_train.shape[1]\n",
    "        self.hidden_layer_size = 25\n",
    "        self.output_layer_size = 10\n",
    "        self.epsilon = []\n",
    "        self.J = 0\n",
    "        self.grad = 0\n",
    "        self.num_labels = 10\n",
    "        self.lambda1 = 0\n",
    "        \n",
    "        \n",
    "    def randInit(self):\n",
    "        theta1 = []\n",
    "        theta2 = []\n",
    "        for x in range(2):\n",
    "            if(x == 0):\n",
    "                L_in = self.input_layer_size\n",
    "                L_out = self.hidden_layer_size\n",
    "            if(x == 1):\n",
    "                L_in = self.hidden_layer_size\n",
    "                L_out = self.output_layer_size\n",
    "            self.epsilon.append(math.sqrt(6) / math.sqrt(L_in + L_out))\n",
    "            \n",
    "        X_bias = self.input_layer_size + 1\n",
    "        \n",
    "        theta1 = (np.random.rand(self.hidden_layer_size,\n",
    "                 self.input_layer_size + 1) * (2 * self.epsilon[0])\n",
    "                 - self.epsilon[0])\n",
    "        theta2 = (np.random.rand(self.output_layer_size,\n",
    "                  self.hidden_layer_size + 1) * (2 * self.epsilon[1])\n",
    "                  - self.epsilon[1]) \n",
    "        #self.theta1 = np.random.randn(self.hidden_layer_size, self.input_layer_size+1)\n",
    "        #self.theta2 = np.random.randn(self.output_layer_size, self.hidden_layer_size+1)\n",
    "        print(theta2[0][:2])\n",
    "        return theta1,theta2\n",
    "    \n",
    "    \n",
    "    def sigmoid(self,z):        \n",
    "        g = 1.0 / (1.0 + np.exp(-z))\n",
    "        return g\n",
    "    \n",
    "    def sigmoidGradient(self,z):\n",
    "        g = 1.0 / (1.0 + np.exp(-z))\n",
    "        g = g*(1-g)\n",
    "        return g\n",
    "    \n",
    "    def nnCostFunction(self,nn_params, input_layer_size, hidden_layer_size, \\\n",
    "\tnum_labels, X, y, lambda_reg):\n",
    "        \n",
    "        theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)], \\\n",
    "                     (hidden_layer_size, input_layer_size + 1), order='F')\n",
    "\n",
    "        theta2 = np.reshape(nn_params[hidden_layer_size * (input_layer_size + 1):], \\\n",
    "                     (num_labels, hidden_layer_size + 1), order='F')\n",
    "        \n",
    "        a1 = np.insert(X, 0, 1, axis=1)\n",
    "        \n",
    "        z2 = np.dot(a1, theta1.T)\n",
    "        \n",
    "        a2 = self.sigmoid(z2)\n",
    "        \n",
    "        a2 = np.insert(a2, 0, 1, axis=1)\n",
    "        \n",
    "        z3 = np.dot(a2, theta2.T)\n",
    "        \n",
    "        a3 = self.sigmoid(z3)\n",
    "        #print(a3.shape,\"Here\")\n",
    "        #print(a3[0][:])\n",
    "        \n",
    "#         yMatrix = (np.tile([y for y in range(10)], (self.m, 1) ) = \n",
    "#         np.tile(y, (1,self.num_labels) ))\n",
    "\n",
    "        numLabels_temp = np.tile([y for y in range(10)], (self.m, 1) )\n",
    "        \n",
    "        yMatrix_temp = np.tile(y, (1,self.num_labels) )\n",
    "        \n",
    "        yMatrix = np.equal(numLabels_temp, yMatrix_temp)\n",
    "        #print( (yMatrix * ((np.log(a3)))).shape, \"First\" )\n",
    "        #print( (((1 - yMatrix.T).T * np.log(1 - a3))).shape )\n",
    "        cost =  ( (yMatrix * (np.log(a3)) ) + ((1 - yMatrix.T).T * np.log(1 - a3)) )\n",
    "        #print(yMatrix.shape, (np.log(a3)).shape, \"Here\" )\n",
    "        #print(cost.shape,\"Cost\")\n",
    "        #print(cost[0])\n",
    "        #print(self.theta1.shape)\n",
    "        #print( (self.theta1[:,1:]).shape)\n",
    "        #print(self.theta1[0][1])\n",
    "        sqTheta1 = np.square(theta1[:,1:]);\n",
    "        #print(sqTheta2[0][0])\n",
    "        #print(self.theta2.shape)\n",
    "        sqTheta2 = np.square(theta2[:,1:]);\n",
    "        #print(sqTheta2.shape);\n",
    "        \n",
    "        self.J = (1/self.m) * cost.sum() + ((self.lambda1/(2*self.m))) * sqTheta1.sum() + \\\n",
    "            sqTheta2.sum();\n",
    "            \n",
    "        print(self.J,\" cost\")\n",
    "        \n",
    "        #Back Prop\n",
    "        delta1 = 0;\n",
    "        delta2 = 0; \n",
    "        #print(a1.shape,\"a1.shape\")\n",
    "        for t in range(0,self.m):\n",
    "            #print(a1.shape)\n",
    "            a1_t = a1[t,]\n",
    "            #print(a1_t.shape)\n",
    "            #print(a1_t)\n",
    "            \n",
    "            a2_t = a2[t,].T\n",
    "            \n",
    "            a3_t = a3[t,].T\n",
    "            \n",
    "            y_output_t = yMatrix[t,].T\n",
    "            \n",
    "            delta3_t = (a3_t - y_output_t);\n",
    "            \n",
    "            z2_t = np.dot(a1_t, theta1.T);\n",
    "            #print(self.theta1.shape, \"self.theta1\")\n",
    "            z2_t= np.insert(z2_t,0 ,1, axis=0)\n",
    "            #print(z2_t.shape)\n",
    "            \n",
    "            delta2_t = np.dot(theta2.T, delta3_t) * self.sigmoidGradient(z2_t)\n",
    "            \n",
    "            delta2_t = delta2_t[1:]\n",
    "            \n",
    "            delta2 = delta2 + np.outer(delta3_t, a2_t)\n",
    "            delta1 = delta1 + np.outer(delta2_t, a1_t.T)\n",
    "        \n",
    "        #theta1_no_bias = (self.lambda1/self.m) * self.theta1[:][2:]\n",
    "        #theta2_no_bias = (self.lambda1/self.m) * self.theta2[:][2:]\n",
    "        \n",
    "        Theta1_grad = delta1 / self.m\n",
    "        Theta2_grad = delta2 / self.m\n",
    "        \n",
    "        Theta1_grad_unregularized = np.copy(Theta1_grad)\n",
    "        Theta2_grad_unregularized = np.copy(Theta2_grad)\n",
    "        Theta1_grad += (float(self.lambda1)/self.m)*theta1\n",
    "        Theta2_grad += (float(self.lambda1)/self.m)*theta2\n",
    "        Theta1_grad[:,0] = Theta1_grad_unregularized[:,0]\n",
    "        Theta2_grad[:,0] = Theta2_grad_unregularized[:,0]\n",
    "        self.grad = np.concatenate((Theta1_grad.reshape(Theta1_grad.size, order='F'), Theta2_grad.reshape(Theta2_grad.size, order='F')))\n",
    "        print(\"hey\")\n",
    "        return self.J,self.grad\n",
    "        \n",
    "    def train(self,theta1,theta2,nn_params, X, y, lambda_reg):\n",
    "        print(self.J,)\n",
    "        print('Training Neural Network...')\n",
    "        #maxiter = 20\n",
    "        maxiter = 5\n",
    "        lambda_reg = 0.1\n",
    "        nn_params = np.concatenate((theta1.reshape(theta1.size, order='F'), theta2.reshape(theta2.size, order='F')))\n",
    "        myargs = (self.input_layer_size, self.hidden_layer_size, self.num_labels, X, y, lambda_reg)\n",
    "        results = minimize(self.nnCostFunction, x0=nn_params, args=myargs, options={'disp': True, 'maxiter':maxiter}, method=\"L-BFGS-B\", jac=True)\n",
    "\n",
    "        nn_params = results[\"x\"]\n",
    "\n",
    "        # Obtain Theta1 and Theta2 back from nn_params\n",
    "        Theta1 = np.reshape(nn_params[:self.hidden_layer_size * (self.input_layer_size + 1)], \\\n",
    "                         (self.hidden_layer_size, self.input_layer_size + 1), order='F')\n",
    "\n",
    "        Theta2 = np.reshape(nn_params[self.hidden_layer_size * (self.input_layer_size + 1):], \\\n",
    "                         (self.num_labels, self.hidden_layer_size + 1), order='F')\n",
    "\n",
    "        print('Program paused. Press enter to continue.\\n')\n",
    "        return Theta1,Theta2\n",
    "        \n",
    "    def predict(self,Theta1, Theta2, X):\n",
    "    #PREDICT Predict the label of an input given a trained neural network\n",
    "    #   p = PREDICT(Theta1, Theta2, X) outputs the predicted label of X given the\n",
    "    #   trained weights of a neural network (Theta1, Theta2)\n",
    "\n",
    "        # turns 1D X array into 2D\n",
    "        if X.ndim == 1:\n",
    "            X = np.reshape(X, (-1,X.shape[0]))\n",
    "\n",
    "        # Useful values\n",
    "        m = X.shape[0]\n",
    "        num_labels = Theta2.shape[0]\n",
    "\n",
    "        # You need to return the following variables correctly \n",
    "        p = np.zeros((m,1))\n",
    "\n",
    "        h1 = self.sigmoid( np.dot( np.column_stack( ( np.ones((m,1)), X ) ) , Theta1.T ) )\n",
    "        h2 = self.sigmoid( np.dot( np.column_stack( ( np.ones((m,1)), h1) ) , Theta2.T ) )\n",
    "\n",
    "        p = np.argmax(h2, axis=1)\n",
    "\n",
    "        # =========================================================================\n",
    "\n",
    "        return p + 1 # offsets python's zero notation\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.16633523  0.18304804]\n",
      "<class 'numpy.ndarray'>\n",
      "(1, 42000)\n",
      "0\n",
      "Training Neural Network...\n",
      "5.850499298722532  cost\n",
      "hey\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keanu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:41: RuntimeWarning: overflow encountered in exp\n",
      "/home/keanu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:45: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.560425167538845  cost\n",
      "hey\n",
      "7.325299518418628  cost\n",
      "hey\n",
      "6.080658113405386  cost\n",
      "hey\n",
      "5.872878099671992  cost\n",
      "hey\n",
      "5.85261046789954  cost\n",
      "hey\n",
      "5.8506973002743745  cost\n",
      "hey\n",
      "5.85051785948578  cost\n",
      "hey\n",
      "5.850501038538566  cost\n",
      "hey\n",
      "5.85049946180564  cost\n",
      "hey\n",
      "5.850499314009255  cost\n",
      "hey\n",
      "5.850499300155446  cost\n",
      "hey\n",
      "5.850499298856848  cost\n",
      "hey\n",
      "5.850499298735118  cost\n",
      "hey\n",
      "5.850499298723712  cost\n",
      "hey\n",
      "5.850499298722642  cost\n",
      "hey\n",
      "5.850499298722543  cost\n",
      "hey\n",
      "5.850499298722532  cost\n",
      "hey\n",
      "5.850499298722532  cost\n",
      "hey\n",
      "5.850499298722539  cost\n",
      "hey\n",
      "5.850499298722532  cost\n",
      "hey\n",
      "Program paused. Press enter to continue.\n",
      "\n",
      "Training Set Accuracy: 10.414286\n"
     ]
    }
   ],
   "source": [
    "nn = Neural_Network(X_train,np.transpose(y_train))\n",
    "theta1 = theta2 = []\n",
    "theta1,theta2 = nn.randInit()\n",
    "nn_params = np.concatenate((theta1.reshape(theta1.size, order='F'), theta2.reshape(theta2.size, order='F')))\n",
    "print(type(y_train))\n",
    "print(y_train.shape)\n",
    "#nn.nnCostFunction(0,0,0,0,X_train,np.transpose(y_train),0)\n",
    "theta1,theta2 = nn.train(theta1,theta2,nn_params,X_train,np.transpose(y_train),0)\n",
    "pred = nn.predict(theta1, theta2, X_train)\n",
    "\n",
    "# uncomment code below to see the predictions that don't match\n",
    "# fmt = '{}   {}'\n",
    "# print(fmt.format('y', 'pred'))\n",
    "# for y_elem, pred_elem in zip(y, pred):\n",
    "#     if y_elem != pred_elem:\n",
    "#         print(fmt.format(y_elem%10, pred_elem%10))\n",
    "\n",
    "print('Training Set Accuracy: {:f}'.format( ( np.mean(pred == y_train)*100 ) ) )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
